# Matrix Multiplication Optimization â€” *The Rancho Way*
![C++](https://img.shields.io/badge/Language-C%2B%2B-blue)
![Architecture](https://img.shields.io/badge/Focus-Computer%20Architecture-orange)
![Optimization](https://img.shields.io/badge/Category-Performance%20Optimization-green)
![Course](https://img.shields.io/badge/CS683-IIT%20Bombay-red)

> *â€œOptimizing computation by understanding how software interacts with hardware.â€*

---

## ğŸ§­ Overview

This project explores **hardware-conscious optimization** for **matrix multiplication** and **embedding operations** â€” two critical components of modern **Machine Learning** and **Deep Learning** workloads.

It demonstrates how thoughtful use of **cache**, **SIMD**, and **software prefetching** can significantly boost performance on Intel x86 architectures.

---

## ğŸ¯ Objectives

- Understand the impact of **cache hierarchies** and **data locality**.
- Apply **loop-level** and **data-level parallelism** to matrix multiplication.
- Leverage **SIMD (Single Instruction, Multiple Data)** for vectorized computation.
- Implement **software prefetching** to reduce cache miss penalties.
- Optimize **Neural Network** and **Embedding** operations using these techniques.
- Measure performance using:
  - L1/L2/LLC Cache Misses  
  - MPKI (Misses Per Kilo Instructions)  
  - Execution Time & Speedup  
  - Instruction Count  

---

## ğŸ§© Project Structure

```
pa1-three-geniuses/
â”œâ”€â”€ part1/                         # Matrix Multiplication Optimizations
â”‚   â”œâ”€â”€ mat_mul/
â”‚   â”‚   â”œâ”€â”€ matrix.c               # Naive + Optimized implementations
â”‚   â”‚   â”œâ”€â”€ Makefile               # Build rules
â”‚   â”‚   â””â”€â”€ mat_mul_analysis.pdf   # Performance analysis report
â”‚   â””â”€â”€ neural_net/                # Neural Network using optimized matmul
â”‚       â”œâ”€â”€ neural_net.cpp
â”‚       â””â”€â”€ neural_net.pdf
â”‚
â”œâ”€â”€ part2/                         # Embedding Operation Optimizations
â”‚   â”œâ”€â”€ emb.cpp                    # Embedding with prefetching + SIMD
â”‚
â”œâ”€â”€ plots/                         # Performance visualizations
â”‚   â”œâ”€â”€ speedup_comparison.png
â”‚   â”œâ”€â”€ cache_effect.pdf
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ README.md                      # This file
```

---

## ğŸ§® Task 1 â€” Matrix Multiplication (*The Rancho Way*)

### ğŸ”¹ Task 1A: Unroll Baba Unroll
- Implemented **loop unrolling** and **loop reordering**.
- Improved **instruction-level parallelism** and **cache hit rate**.
- Evaluated performance via MPKI and execution time.

### ğŸ”¹ Task 1B: Divide Karo, Rule Karo
- Implemented **Tiled Matrix Multiplication** for cache efficiency.
- Experimented with different **tile sizes**.
- Measured:
  - L1-D Cache MPKI vs. Matrix Size  
  - Speedup vs. Matrix Size  

### ğŸ”¹ Task 1C: Data Ko Line Mein Lagao
- Introduced **SIMD (SSE/AVX/AVX512)** vectorization.
- Used Intel intrinsics (`_m128d`, `_m256d`, `_m512d`).
- Observed up to **2â€“3Ã— speedup** through parallel arithmetic.

### ğŸ”¹ Task 1D: Ranchoâ€™s Final Year Project
- Combined **Tiling + SIMD + Loop Unrolling**.
- Demonstrated **synergistic performance improvement** over individual methods.

### ğŸ”¹ Task 1E: Confusion hi Confusion Hai !!
- Applied optimized matrix multiplication inside a **Neural Network**.
- Measured end-to-end inference speedup.

---

## ğŸ’¡ Task 2 â€” Embed It (Embedding Optimization)

### ğŸ”¹ Task 2A: Software Prefetching
- Used `_mm_prefetch` and `_builtin_prefetch` for memory access optimization.
- Tuned:
  - **Prefetch Distance**
  - **Cache Fill Level**
- Analyzed **L1/L2/LLC Misses**, execution time, and **speedup**.

### ğŸ”¹ Task 2B: SIMD
- Parallelized embedding summations using **SIMD intrinsics**.
- Compared multiple **SIMD widths** (64/128/256 bits).
- Evaluated **instruction count** and **execution time** improvements.

### ğŸ”¹ Task 2C: Software Prefetching + SIMD
- Combined both optimizations.
- Achieved the **highest speedup** for large embedding tables.
- Plotted performance trends for all methods.

---

## ğŸ“Š Results & Insights

| Technique | Focus | Typical Speedup | Observation |
|------------|--------|----------------|--------------|
| Loop Unrolling | Instruction-level parallelism | ~1.2Ã— | Reduced loop overhead |
| Tiling | Cache reuse | ~1.8Ã— | Improved spatial locality |
| SIMD | Data-level parallelism | ~2â€“3Ã— | Parallel multiply-add |
| Prefetching | Memory latency hiding | ~1.3Ã— | Fewer cache stalls |
| Combined (Tiling + SIMD + Prefetch) | Full-stack optimization | ~4â€“5Ã— | Hardware-aware synergy |

---

## ğŸ§° Tools & Environment

- **Language:** C / C++
- **Profiler:** [`perf`](https://perfwiki.github.io/main/)
- **Hardware:** Intel x86 CPU with AVX / AVX2 / AVX512 support
- **Libraries:**
  - `<immintrin.h>` for SIMD intrinsics
  - GCC/Clang (compile with `-O0` for fair measurement)

---

## ğŸ“ˆ Sample Visualizations

| Plot | Description |
|------|--------------|
| ![Speedup Comparison](plots/speedup_comparison.png) | Speedup vs Matrix Size |
| ![Cache Effect](plots/cache_effect.pdf) | Cache Misses vs Tile Size |

---

## ğŸ§  Key Takeaways

1. **Cache-aware programming** greatly improves data reuse.  
2. **SIMD** enables parallel arithmetic on multiple elements per cycle.  
3. **Software prefetching** hides memory latency by anticipating data access.  
4. **Combining techniques** delivers super-linear performance benefits.  
5. These optimizations directly apply to **ML and DL kernels** like GEMM, convolution, and embedding lookup.

---

## âš™ï¸ How to Run

```bash
# Compile all targets
make

# Run baseline implementation
./matrix_naive

# Run optimized implementation
./matrix_optimized

# Profile with perf
perf stat -e cache-misses,cache-references ./matrix_optimized
```

---

## ğŸ‘¨â€ğŸ’» Authors

**Team:** OneCpi
**Course:** CS683 â€” Advanced Computer Architecture
**Institution:** Indian Institute of Technology, Bombay  
**Instructor:** Prof. Biswabandan Panda

---

## ğŸ§¾ References

- [Intel Intrinsics Guide](https://www.intel.com/content/www/us/en/docs/intrinsics-guide)
- [perf Wiki](https://perfwiki.github.io/main/)
- [CASPER Group](https://casper-iitb.github.io/)

---

> ğŸ“˜ *This project demonstrates how a deep understanding of computer architecture enables smarter, faster, and more efficient software â€” especially in ML and HPC workloads.*
